{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f92f1cc-ff2c-4805-8495-d28deed3b381",
   "metadata": {},
   "source": [
    "# Buidling a dataset\n",
    "\n",
    "We're going to fetch and clean some data about how people use search. This stuff is tracked in real time by our reporting stack, but we want a static representative dataset for reproducible analysis here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5848880-cbaa-45f9-a7ab-fe422b36f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f841375-7470-4f58-a026-c332674cce86",
   "metadata": {},
   "source": [
    "We need to create a client in order to interact with the reporting indexes in elastic cloud, and we'll use a couple of secrets from our project's environment to get access. Vercel adds quotes around variables in the `.env` file, so we'll strip those out with a quick lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dface6ea-8caf-4d56-aa6a-26430549d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = lambda name: os.environ[name][1:-1]\n",
    "\n",
    "es = Elasticsearch(\n",
    "    cloud_id=secret(\"ES_REPORTING_CLOUD_ID\"),\n",
    "    http_auth=(secret(\"ES_REPORTING_USER\"), secret(\"ES_REPORTING_PASSWORD\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924a1d4-db3d-41f9-94ec-7d50aaa9a78a",
   "metadata": {},
   "source": [
    "We'll use our client to get data from the `conversion` index. That index tracks a lot of stuff, but all we're interested in are the searches, ie documents where a set of search terms exists in the `\"page.query.query\"` field.\n",
    "\n",
    "We'll also restrict our query to a known date range, and only include searches from the `/works` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417648f7-efa8-432a-b132-df8fc708b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = es.search(\n",
    "    index=\"conversion\",\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\n",
    "                        \"exists\": {\n",
    "                            \"field\": \"page.query.query\",\n",
    "                            \"field\": \"properties.totalResults\",\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"@timestamp\": {\"gte\": \"2021-01-01\", \"lt\": \"2021-02-01\"}\n",
    "                        }\n",
    "                    },\n",
    "                    {\"term\": {\"page.name\": \"works\"}},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    _source=[\"page.query.query\", \"properties.totalResults\", \"@timestamp\"],\n",
    "    size=100_000,\n",
    "    request_timeout=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923306a-93f3-4866-ac63-8fc14bab511a",
   "metadata": {},
   "source": [
    "Next we'll clean up the raw data from the elasticsearch response. Working with `pandas` makes this process quite neat and readable. \n",
    "\n",
    "We're flattening the data, adding some more readable column headings, and stripping out the times from the timestamps - we might want to know which query we were running when these searches were originally run, so the date is worth keeping, but we don't need the precise second. The date gives us what we need while minimising the risk of tying searches back to individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ff503-3b79-4d32-97fe-849c6740f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"search_terms\": hit[\"_source\"][\"page\"][\"query\"][\"query\"],\n",
    "            \"n_results\": hit[\"_source\"][\"properties\"][\"totalResults\"],\n",
    "            \"date\": hit[\"_source\"][\"@timestamp\"],\n",
    "        }\n",
    "        for hit in resp[\"hits\"][\"hits\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdfe7e2-7bd4-4db9-862e-2a8e307bc143",
   "metadata": {},
   "source": [
    "Finally, we'll dump our dataset into a csv which we can reuse later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f6edf-28fd-4f29-9c52-1494bda758e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./searches.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9984987-4078-46bc-b268-528f802e88e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
