{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we\n",
    "- Take all 120588 images from the S3 bucket `wellcomecollection-miro-images-public`.\n",
    "- We read the image, resize it to having a maximum length or width of 224, and convert any black and white images to RGB.\n",
    "- These images are then saved in batches to `storage/data`, with the prefix 'processed_images_batch_'.\n",
    "- We also take a look at how big all the images were, in which we found some of the images were so big that it was worth not saving them in batches, but rather each in a separate png file, which would make reading easier. Thus, we read the previous batches of processed images and saved each image as a png, e.g. A0000001.png\n",
    "- Due to some image errors 4 of the batches didn't work, so the images from these are saved individually in pngs at the end.\n",
    "- `storage/data` now contains 120576 images as pngs\n",
    "\n",
    "Note: 11 images didn't save, the names of these are saved in \"../data/images_not_saved\" and are:\n",
    "['B0008000/B0008543.jpg','B0008000/B0008573.jpg','B0009000/B0009632.jpg','B0010000/B0010992.jpg','L0038000/L0038247.jpg',\n",
    " 'L0083000/L0083878.jpg','L0086000/L0086135.jpg','L0086000/L0086136.jpg','Large Files/L0078598.jpg','Large Files/L0080109.jpg','Large Files/L0080110.jpg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get image names from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'wellcomecollection-miro-images-public'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = boto3.client('sts')\n",
    "assumed_role_object = sts.assume_role(\n",
    "    RoleArn='arn:aws:iam::760097843905:role/calm-assumable_read_role',\n",
    "    RoleSessionName='AssumeRoleSession1'\n",
    ")\n",
    "credentials = assumed_role_object['Credentials']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_fetch = boto3.resource('s3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3_fetch.Bucket(bucket_name)\n",
    "bucket_info = bucket.meta.client.list_objects(\n",
    "    Bucket=bucket.name,\n",
    "    Delimiter='/'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folder names.\n",
    "folder_names = [\n",
    "    f['Prefix'] for f in bucket_info.get('CommonPrefixes')\n",
    "    ]\n",
    "print(\"{} image folders\".format(len(folder_names))) # 219\n",
    "\n",
    "# Get all file dirs from all folders. Takes a minute or so\n",
    "print(\"Getting all file dir names for all images...\")\n",
    "file_dirs = []\n",
    "for folder_name in tqdm(folder_names):\n",
    "    file_dirs.extend(\n",
    "        [s.key for s in bucket.objects.filter(Prefix=folder_name)]\n",
    "    )\n",
    "print(\"{} image files\".format(len(file_dirs))) # 120589"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(file_dir, bucket_name):\n",
    "    \n",
    "    obj = s3_fetch.Object(\n",
    "        bucket_name,\n",
    "        file_dir\n",
    "        )\n",
    "    im = Image.open(BytesIO(obj.get()['Body'].read()))\n",
    "    im.thumbnail((224, 224))\n",
    "    if im.mode != \"RGB\":\n",
    "        im = im.convert('RGB')\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chrisalbon.com/python/data_wrangling/break_list_into_chunks_of_equal_size/\n",
    "# Create a function called \"chunks\" with two arguments, l and n:\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "batches = chunks(file_dirs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_prefix = \"processed_images_batch_\"\n",
    "filenames = os.listdir('../data/')\n",
    "batch_numbers_completed = [\n",
    "    int(os.path.splitext(filename)[0].replace(filename_prefix, \"\")) for filename in filenames if filename_prefix in filename\n",
    "    ]\n",
    "\n",
    "print(\"{} batches completed out of {}\".format(len(batch_numbers_completed),round(len(file_dirs)/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes a long time and sometimes there are errors, so I will process and save in batches\n",
    "\n",
    "for i, batch in tqdm(enumerate(batches)):\n",
    "    if not i in batch_numbers_completed:\n",
    "        print(i)\n",
    "        try:\n",
    "            batch_images = {\n",
    "                os.path.splitext(\n",
    "                    os.path.basename(file_dir)\n",
    "                )[0]: get_image(file_dir, bucket_name) for file_dir in batch\n",
    "            }\n",
    "            with open('../data/processed_images_batch_{}.pkl'.format(i), 'wb') as handle:\n",
    "                pickle.dump(batch_images, handle)\n",
    "        except:\n",
    "            print(\"Issue with batch {}\".format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) how big are the images files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '../data/'\n",
    "file_name_start = 'processed_images_batch'\n",
    "\n",
    "image_file_names = os.listdir(file_dir)\n",
    "image_file_names = [file for file in image_file_names if file_name_start in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {}\n",
    "for image_file_name in tqdm(image_file_names):\n",
    "    statinfo = os.stat(file_dir + image_file_name)\n",
    "    sizes[image_file_name] = statinfo.st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(sizes.values()), bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(sizes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(sizes.values())) # 34GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The batch pickles are too big to load and keep in memory\n",
    "\n",
    "I individually save them into pngs, which means I can access one at a time.\n",
    "\n",
    "This is more useful since I only need one at a time to get the feature vectors, and I \n",
    "don't need to plot them all in one go (it'd be a mess anyway).\n",
    "\n",
    "For the pathways I only need a few of the images at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '../data/'\n",
    "file_name_start = 'processed_images_batch'\n",
    "\n",
    "data_file_names = os.listdir(file_dir)\n",
    "batch_image_file_names = [file for file in data_file_names if (file_name_start in file and '.pkl' in file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_file_name in tqdm(image_file_names):\n",
    "    with open(file_dir + image_file_name, 'rb') as handle:\n",
    "        image_batch = pickle.load(handle)\n",
    "        for image_name, image in image_batch.items():\n",
    "            if not image_name+\".png\" in data_file_names:\n",
    "                image.save(file_dir + image_name + \".png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get the images for the files in the 4 batches that didn't work in step 2\n",
    "\n",
    "Find the image names that didn't get saved in batches and individually save them as pngs.\n",
    "\n",
    "Need to run the first part of step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_batch_numbers = [i for i in range(0,round(len(file_dirs)/batch_size)) if i not in batch_numbers_completed]\n",
    "incomplete_batch_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_batch_image_file_names = []\n",
    "for i, batch in enumerate(batches):\n",
    "    if i in incomplete_batch_numbers:\n",
    "        print(i)\n",
    "        bad_batch_image_file_names.extend(list(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_batch_image_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_images = []\n",
    "good_images = []\n",
    "for image_dir in tqdm(bad_batch_image_file_names):\n",
    "    try:\n",
    "        image = get_image(image_dir, bucket_name)\n",
    "        image_name = os.path.splitext(\n",
    "            os.path.basename(image_dir)\n",
    "            )[0]\n",
    "        image.save(file_dir + image_name + \".png\")\n",
    "        good_images.append(image_dir)\n",
    "    except:\n",
    "        bad_images.append(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(good_images))\n",
    "print(len(bad_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/images_not_saved\", np.array(bad_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
