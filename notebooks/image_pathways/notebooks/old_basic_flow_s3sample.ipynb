{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import vgg\n",
    "import torchvision.transforms as transforms\n",
    "import requests\n",
    "import os\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from itertools import combinations \n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg.vgg16(pretrained=True)\n",
    "vgg16 = vgg16.eval()  # for no dropout behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "\n",
    "# Let's get our class labels for this model.\n",
    "response = requests.get(LABELS_URL)  # Make an HTTP GET request and store the response.\n",
    "labels = {int(key): value for key, value in response.json().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get images from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'wellcomecollection-miro-images-public'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = boto3.client('sts')\n",
    "assumed_role_object = sts.assume_role(\n",
    "    RoleArn='arn:aws:iam::760097843905:role/calm-assumable_read_role',\n",
    "    RoleSessionName='AssumeRoleSession1'\n",
    ")\n",
    "credentials = assumed_role_object['Credentials']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket_info = bucket.meta.client.list_objects(\n",
    "    Bucket=bucket.name,\n",
    "    Delimiter='/'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folder names.\n",
    "folder_names = [\n",
    "    f['Prefix'] for f in bucket_info.get('CommonPrefixes')\n",
    "    ]\n",
    "print(\"{} image folders\".format(len(folder_names))) # 219\n",
    "\n",
    "# Get all file dirs from all folders. Takes a minute or so\n",
    "print(\"Getting all file dir names for all images...\")\n",
    "file_dir = []\n",
    "for folder_name in tqdm(folder_names):\n",
    "    file_dir.extend(\n",
    "        [s.key for s in bucket.objects.filter(Prefix=folder_name)]\n",
    "    )\n",
    "print(\"{} image files\".format(len(file_dir))) # 120589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick n random image directories and store them\n",
    "n = 1000\n",
    "np.random.seed(seed=0) # Just for dev\n",
    "random_file_dir = np.random.choice(file_dir, n, replace=False)\n",
    "\n",
    "print(\"Storing {} random images...\".format(n))\n",
    "images = []\n",
    "for file in tqdm(random_file_dir):\n",
    "    obj = s3.Object(bucket_name, file)\n",
    "    im = Image.open(BytesIO(obj.get()['Body'].read()))\n",
    "    im.thumbnail((750, 750))\n",
    "    if im.mode != \"RGB\":\n",
    "        im = im.convert('RGB')\n",
    "    images.append(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict image (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
    "transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need to re run if you overwrite it with vgg16_short\n",
    "vgg16 = vgg.vgg16(pretrained=True)\n",
    "vgg16 = vgg16.eval()  # for no dropout behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(transform_pipeline, im, model, labels):\n",
    "    \n",
    "    img = transform_pipeline(im)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    # Now let's get a prediciton!\n",
    "    prediction = model(img)  # Returns a Tensor of shape (batch, num class labels)\n",
    "    prediction = prediction.data.numpy().argmax()  # Our prediction will be the index of the class label with the largest value.\n",
    "    print(prediction)\n",
    "    return labels[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = images[5]\n",
    "print(predict_image(transform_pipeline, im, vgg16, labels))\n",
    "im.resize((200,200), resample= Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract feature vectors from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
    "transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last layer from the model, so that the output will be a feature vector\n",
    "vgg16_short = vgg16\n",
    "vgg16_short.classifier = vgg16.classifier[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting feature vectors for {} images...\".format(len(images)))\n",
    "feature_vectors = []\n",
    "for i, image in tqdm(enumerate(images)):\n",
    "    img = transform_pipeline(image)\n",
    "    img = img.unsqueeze(0)\n",
    "    feature_vectors.append(vgg16_short(img).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get the pairwise distance matrix for the images, and the closest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = cdist(feature_vectors, feature_vectors, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat_top = np.zeros_like(dist_mat)\n",
    "dist_mat_top[:]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "# Find the top n neighbours for each image\n",
    "\n",
    "for i, _ in tqdm(enumerate(images)):\n",
    "    arr = dist_mat[i].argsort()\n",
    "    top_args = arr[arr!=i]\n",
    "    dist_mat_top[i][top_args[0:n]] = dist_mat[i][top_args[0:n]]\n",
    "    for j in top_args[0:n]:\n",
    "        dist_mat_top[j][i] = dist_mat[j][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the network of images connected to their closest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_rel_norm(value, min_val, max_val):\n",
    "    value = (value - min_val)/(max_val - min_val)\n",
    "    value = 1/(value+1e-8)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(dist_mat_top):\n",
    "    \n",
    "    min_val = np.nanmin(dist_mat_top)\n",
    "    max_val = np.nanmax(dist_mat_top)\n",
    "    \n",
    "    nodes = list(range(0,len(dist_mat_top[0])))\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "\n",
    "    # Put the weights in as the distances\n",
    "    # only inc nodes if they are in the closest related neighbours\n",
    "    for start, end in list(combinations(nodes, 2)):\n",
    "        if ~np.isnan(dist_mat_top[start, end]):\n",
    "            # Since in the plot a higher weight makes the nodes closer, \n",
    "            # but a higher value in the distance matrix means the images are further away,\n",
    "            # we need to inverse the weight (so higher = closer)\n",
    "            G.add_edge(\n",
    "                start,\n",
    "                end,\n",
    "                weight=inv_rel_norm(dist_mat_top[start, end], min_val, max_val)\n",
    "            )\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(G, image_names=None):\n",
    "    \n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    plt.figure(3,figsize=(10,10)) \n",
    "    nx.draw(G, pos, node_size=10)\n",
    "    for p in pos:  # raise text positions\n",
    "        pos[p][1] += 0.06\n",
    "    if image_names:\n",
    "        image_names_dict = {k:str(k)+\" \"+v for k,v in enumerate(image_names)}\n",
    "        nx.draw_networkx_labels(G, pos, labels=image_names_dict)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_graph(dist_mat_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualise the clusters by reducing dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "embedding_fv = reducer.fit_transform(feature_vectors)\n",
    "embedding_fv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/gaborvecsei/plants-t-sne\n",
    "def visualize_scatter_with_images(X_2d_data, images, figsize=(45,45), image_zoom=1):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    artists = []\n",
    "    for xy, i in zip(X_2d_data, images):\n",
    "        x0, y0 = xy\n",
    "        img = OffsetImage(i, zoom=image_zoom)\n",
    "        ab = AnnotationBbox(img, (x0, y0), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(X_2d_data)\n",
    "    ax.autoscale()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[a, b] for (a,b) in zip(embedding_fv[:, 0], embedding_fv[:, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_scatter_with_images(\n",
    "    x_data,\n",
    "    images = images,\n",
    "    image_zoom=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of the biggest differences between 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat_top[262]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanargmax(dist_mat_top, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pick 2 images and look at the route between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names_dict = {k:v for k,v in enumerate(random_file_dir)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = np.random.choice(list(image_names_dict))\n",
    "node2 = np.random.choice(list(image_names_dict))\n",
    "\n",
    "# nice path:\n",
    "# node1 = 6 \n",
    "# node2 = 146\n",
    "\n",
    "node_path = nx.dijkstra_path(G, node1, node2, weight=None)\n",
    "print(node_path)\n",
    "\n",
    "show_images = [images[i] for i in node_path]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "columns = len(show_images)\n",
    "for i, image in enumerate(show_images):\n",
    "    ax = plt.subplot(len(show_images) / columns + 1, columns, i + 1)\n",
    "    ax.set_axis_off()\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
