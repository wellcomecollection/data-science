{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import vgg\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg.vgg16(pretrained=True)\n",
    "vgg16 = vgg16.eval()  # for no dropout behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_URL = \"https://s3.amazonaws.com/outcome-blog/imagenet/labels.json\"\n",
    "\n",
    "# Let's get our class labels for this model.\n",
    "response = requests.get(LABELS_URL)  # Make an HTTP GET request and store the response.\n",
    "labels = {int(key): value for key, value in response.json().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher res images straight from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"wellcomecollection-miro-images-public\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = boto3.client(\"sts\")\n",
    "assumed_role_object = sts.assume_role(\n",
    "    RoleArn=\"arn:aws:iam::760097843905:role/calm-assumable_read_role\",\n",
    "    RoleSessionName=\"AssumeRoleSession1\",\n",
    ")\n",
    "credentials = assumed_role_object[\"Credentials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_fetch = boto3.resource(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=credentials[\"AccessKeyId\"],\n",
    "    aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
    "    aws_session_token=credentials[\"SessionToken\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3_fetch.Bucket(bucket_name)\n",
    "bucket_info = bucket.meta.client.list_objects(Bucket=bucket.name, Delimiter=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folder names.\n",
    "folder_names = [f[\"Prefix\"] for f in bucket_info.get(\"CommonPrefixes\")]\n",
    "print(\"{} image folders\".format(len(folder_names)))  # 219\n",
    "\n",
    "# Get all file dirs from all folders. Takes a minute or so\n",
    "print(\"Getting all file dir names for all images...\")\n",
    "file_dirs = []\n",
    "for folder_name in tqdm(folder_names):\n",
    "    file_dirs.extend([s.key for s in bucket.objects.filter(Prefix=folder_name)])\n",
    "print(\"{} image files\".format(len(file_dirs)))  # 120589"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get one image, or an high res image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"V0001893\"  # ['V0001893', 'V0047369EL']\n",
    "file_dir = [f for f in file_dirs if image_name in f][0]\n",
    "\n",
    "obj = s3_fetch.Object(bucket_name, file_dir)\n",
    "im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
    "file_name = os.path.splitext(os.path.basename(file_dir))[0]\n",
    "im\n",
    "im.save(\"../medium_blog_images/{}.png\".format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    os.path.splitext(os.path.basename(f))[0] for f in np.random.choice(file_dirs, 11)\n",
    "] + [\"A0000001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    \"L0061160\",\n",
    "    \"L0038847\",\n",
    "    \"B0006893\",\n",
    "    \"V0010192EL\",\n",
    "    \"V0025035\",\n",
    "    \"L0052856\",\n",
    "    \"V0050358\",\n",
    "    \"L0008713\",\n",
    "    \"V0007884EL\",\n",
    "    \"M0012095\",\n",
    "    \"V0010104\",\n",
    "    \"A0000001\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {}\n",
    "plot_images_sizes = []\n",
    "for image_name in tqdm(image_names):\n",
    "    file = [f for f in file_dirs if image_name in f][0]\n",
    "    obj = s3_fetch.Object(bucket_name, file)\n",
    "    im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
    "    if im.mode != \"RGB\":\n",
    "        im = im.convert(\"RGB\")\n",
    "    im.thumbnail((224, 224), resample=Image.BICUBIC)\n",
    "    plot_images_sizes.append(im.size)\n",
    "    images[image_name] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-row image\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "columns = 6\n",
    "for i, (image_name, im) in enumerate(images.items()):\n",
    "    ax = plt.subplot(np.ceil(len(image_names) / columns), columns, i + 1)\n",
    "    # plt.title(image_name)\n",
    "    ax.set_axis_off()\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_path(image_names, columns=len(image_names)):\n",
    "\n",
    "    images = {}\n",
    "    plot_images_sizes = []\n",
    "    for image_name in tqdm(image_names):\n",
    "        file = [f for f in file_dirs if image_name in f][0]\n",
    "        obj = s3_fetch.Object(bucket_name, file)\n",
    "        im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
    "        if im.mode != \"RGB\":\n",
    "            im = im.convert(\"RGB\")\n",
    "        im.thumbnail((224, 224), resample=Image.BICUBIC)\n",
    "        plot_images_sizes.append(im.size)\n",
    "        images[image_name] = im\n",
    "\n",
    "    max_y = max([c[1] for c in plot_images_sizes])\n",
    "    rescale_x = [c[0] * max_y / c[1] for c in plot_images_sizes]\n",
    "    columns = len(image_names)\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    gs = gridspec.GridSpec(1, columns, width_ratios=rescale_x)\n",
    "\n",
    "    for i, (image_name, im) in enumerate(images.items()):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        ax.set_axis_off()\n",
    "        plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    \"B0008895\",\n",
    "    \"N0021591\",\n",
    "    \"B0007199\",\n",
    "    \"A0001358\",\n",
    "    \"V0007108\",\n",
    "    \"V0036001\",\n",
    "    \"V0037737\",\n",
    "    \"V0026902EL\",\n",
    "    \"M0010374\",\n",
    "]\n",
    "print_path(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\"V0001893\", \"V0047369EL\"]\n",
    "print_path(image_names, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\"B0000663\", \"V0014173\"]\n",
    "print_path(image_names, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\"L0078444\", \"L0078481\"]\n",
    "print_path(image_names, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\"V0003760\", \"V0006594\"]\n",
    "print_path(image_names, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One row image\n",
    "image_names = [\n",
    "    \"A0000001\",\n",
    "    \"A0000002\",\n",
    "    \"A0000003\",\n",
    "    \"A0001260\",\n",
    "    \"B0007248\",\n",
    "    \"B0004589\",\n",
    "    \"B0004848\",\n",
    "    \"B0006893\",\n",
    "]\n",
    "print_path(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    \"A0000785\",\n",
    "    \"B0001152\",\n",
    "    \"A0000318\",\n",
    "    \"V0007884EL\",\n",
    "    \"L0027241\",\n",
    "    \"V0013859\",\n",
    "    \"V0013040\",\n",
    "    \"V0040933\",\n",
    "]\n",
    "print_path(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    \"V0044783\",\n",
    "    \"V0022904ER\",\n",
    "    \"V0021741\",\n",
    "    \"V0021867\",\n",
    "    \"V0021857\",\n",
    "    \"V0023111\",\n",
    "    \"V0023117\",\n",
    "]\n",
    "print_path(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    \"V0044783\",\n",
    "    \"V0022904ER\",\n",
    "    \"V0021741\",\n",
    "    \"A0000113\",\n",
    "    \"B0004207\",\n",
    "    \"V0043888\",\n",
    "    \"V0023376\",\n",
    "    \"V0046793\",\n",
    "]\n",
    "print_path(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    \"V0001893\",\n",
    "    #  'V0003665',\n",
    "    \"V0026311\",\n",
    "    \"V0031656\",\n",
    "    \"V0007101ER\",\n",
    "    \"L0027175\",\n",
    "    \"V0042795EL\",\n",
    "    \"V0044410\",\n",
    "    \"V0042799EL\",\n",
    "    \"V0047369EL\",\n",
    "]\n",
    "print_path(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    \"L0032287\",\n",
    "    \"M0012716\",\n",
    "    \"V0049671\",\n",
    "    \"M0006130\",\n",
    "    \"L0040595\",\n",
    "    \"L0056834\",\n",
    "    \"V0030245\",\n",
    "    \"V0029003\",\n",
    "    \"L0034782\",\n",
    "    \"A0000632\",\n",
    "]\n",
    "print_path(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [\n",
    "    \"L0045857\",\n",
    "    \"L0045858\",\n",
    "    \"L0045856\",\n",
    "    \"L0045886\",\n",
    "    \"V0005269\",\n",
    "    \"V0032946ER\",\n",
    "    \"V0033137EL\",\n",
    "    \"V0035635ER\",\n",
    "    \"V0035703\",\n",
    "    \"V0035629ER\",\n",
    "]\n",
    "print_path(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_img_size = (\n",
    "    224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
    ")\n",
    "transform_pipeline = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(min_img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg.vgg16(pretrained=True)\n",
    "vgg16 = vgg16.eval()  # for no dropout behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(transform_pipeline, im, model, labels):\n",
    "\n",
    "    img = transform_pipeline(im)\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    # Now let's get a prediction!\n",
    "    prediction = model(img)  # Returns a Tensor of shape (batch, num class labels)\n",
    "    return labels[prediction.data.numpy().argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"B0008895\"\n",
    "file_dir = [f for f in file_dirs if image_name in f][0]\n",
    "\n",
    "obj = s3_fetch.Object(bucket_name, file_dir)\n",
    "im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
    "if im.mode != \"RGB\":\n",
    "    im = im.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_image(transform_pipeline, im, vgg16, labels))\n",
    "im.resize((200, 200), resample=Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transform_pipeline(im)\n",
    "img = img.unsqueeze(0)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output the FV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the images transforms\n",
    "min_img_size_fv = (\n",
    "    224,\n",
    "    224,\n",
    ")  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
    "transform_pipeline_fv = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(min_img_size_fv),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "# Remove the last layer from the model, so that the output will be a feature vector\n",
    "vgg16_short = vgg16\n",
    "vgg16_short.classifier = vgg16.classifier[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"M0010374\"\n",
    "file_dir = [f for f in file_dirs if image_name in f][0]\n",
    "\n",
    "obj = s3_fetch.Object(bucket_name, file_dir)\n",
    "im = Image.open(BytesIO(obj.get()[\"Body\"].read()))\n",
    "if im.mode != \"RGB\":\n",
    "    im = im.convert(\"RGB\")\n",
    "\n",
    "img = transform_pipeline(im)\n",
    "img = img.unsqueeze(0)\n",
    "vgg16_short(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
