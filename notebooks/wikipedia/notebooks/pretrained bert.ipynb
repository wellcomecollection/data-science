{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForTokenClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, we know that bert works and will produce some beautiful embeddings which we can fine tune. now we need to put together the training data for the embeddings, using the bert tokeniser\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "base_path = '/mnt/efs/wikipedia/dumps/text/'\n",
    "paths = np.random.choice(os.listdir(base_path), size=10)\n",
    "\n",
    "all_text = ''\n",
    "for path in paths:\n",
    "    filenames = os.listdir(base_path + path)\n",
    "    for filename in tqdm(filenames):\n",
    "        with open(base_path + path + '/' + filename, 'rb') as f:\n",
    "            all_text += f.read().decode('latin1')\n",
    "\n",
    "pattern = r'(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)'\n",
    "articles = [\n",
    "    '\\n\\n'.join(article[0].split('\\n\\n')[1:])\n",
    "    for article in re.findall(pattern, all_text)\n",
    "]\n",
    "\n",
    "#articles = np.random.choice(articles, size=1000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def label_linkable_tokens(sentence, tokenizer, label_all=True):\n",
    "    parsed_html = BeautifulSoup(sentence, 'html.parser')\n",
    "\n",
    "    link_text = [link.text for link in parsed_html.find_all('a')]\n",
    "    tokenised_links = [tokenizer.tokenize(link) for link in link_text]\n",
    "    tokenised_text = tokenizer.tokenize(parsed_html.text)\n",
    "    target_sequence = np.zeros(len(tokenised_text))\n",
    "\n",
    "    for link in tokenised_links:\n",
    "        start_positions = kmp(tokenised_text, link)\n",
    "        if label_all:            \n",
    "            for pos in start_positions:\n",
    "                target_sequence[pos : pos + len(link)] = 1\n",
    "        elif label_all == False and len(start_positions) > 0:\n",
    "            pos = start_positions[0]\n",
    "            target_sequence[pos : pos + len(link)] = 1\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    token_sequence = tokenizer.convert_tokens_to_ids(tokenised_text)\n",
    "    return token_sequence, target_sequence\n",
    "\n",
    "\n",
    "def kmp(sequence, sub):\n",
    "    \"\"\"         \n",
    "    Knuth–Morris–Pratt algorithm, returning the starting position\n",
    "    of a specified subsequence within another, larger sequence.\n",
    "    Usually used for string matching.\n",
    "    \"\"\"\n",
    "    partial = [0]\n",
    "    for i in range(1, len(sub)):\n",
    "        j = partial[i - 1]\n",
    "        while j > 0 and sub[j] != sub[i]:\n",
    "            j = partial[j - 1]\n",
    "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
    "\n",
    "    positions, j = [], 0\n",
    "    for i in range(len(sequence)):\n",
    "        while j > 0 and sequence[i] != sub[j]:\n",
    "            j = partial[j - 1]\n",
    "        if sequence[i] == sub[j]: j += 1\n",
    "        if j == len(sub): \n",
    "            positions.append(i - (j - 1))\n",
    "            j = 0\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "token_sequences, target_sequences = [], []\n",
    "\n",
    "for i, article in enumerate(tqdm(articles)):\n",
    "    if i % 1000 == 0: print(i)\n",
    "    try:\n",
    "        tokenized_sequence, target_sequence = label_linkable_tokens(article, tokenizer)        \n",
    "        token_sequences.append(tokenized_sequence)\n",
    "        target_sequences.append(target_sequence)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pickle.dump(token_sequences, open('/mnt/efs/wikipedia/token_sequences.pkl', 'wb'))\n",
    "pickle.dump(target_sequences, open('/mnt/efs/wikipedia/target_sequences.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequences = pickle.load(open(\"/mnt/efs/wikipedia/token_sequences.pkl\", \"rb\"))\n",
    "target_sequences = pickle.load(open(\"/mnt/efs/wikipedia/target_sequences.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(token_sequences))\n",
    "output_html = \"\"\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_sequences[i])\n",
    "targets = target_sequences[i]\n",
    "for token, target in zip(tokens, targets):\n",
    "    if target == 1:\n",
    "        output_html += f\"<b>{token}</b> \"\n",
    "    else:\n",
    "        output_html += token + \" \"\n",
    "\n",
    "display(HTML(output_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, token_sequences, target_sequences):\n",
    "        where_big_enough = np.where([len(seq) > 10 for seq in token_sequences])\n",
    "        self.token_sequences = np.array(token_sequences)[where_big_enough]\n",
    "        self.target_sequences = np.array(target_sequences)[where_big_enough]\n",
    "        self.lim = 512\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        token_sequence = self.token_sequences[index]\n",
    "        target_sequence = self.target_sequences[index]\n",
    "\n",
    "        # if the sequence is too long for the model to handle,\n",
    "        # grab a random chunk of acceptable length instead\n",
    "        if len(token_sequence) > self.lim:\n",
    "            start_ix = len(token_sequence) - np.random.choice(self.lim)\n",
    "            token_sequence = token_sequence[start_ix : start_ix + self.lim]\n",
    "            target_sequence = target_sequence[start_ix : start_ix + self.lim]\n",
    "\n",
    "        tokens = torch.LongTensor(token_sequence)\n",
    "        targets = torch.LongTensor(target_sequence)\n",
    "        return tokens, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    token_sequences, target_sequences = zip(*batch)\n",
    "    seq_lens = torch.LongTensor([len(seq) for seq in token_sequences])\n",
    "    sorted_lens, sort_indicies = seq_lens.sort(dim=0, descending=True)\n",
    "\n",
    "    sorted_tokens = [token_sequences[i] for i in sort_indicies]\n",
    "    sorted_targets = [target_sequences[i] for i in sort_indicies]\n",
    "\n",
    "    padded_tokens = pad_sequence(\n",
    "        sequences=sorted_tokens, padding_value=0, batch_first=True\n",
    "    )\n",
    "\n",
    "    padded_targets = pad_sequence(\n",
    "        sequences=sorted_targets, padding_value=0, batch_first=True\n",
    "    )\n",
    "\n",
    "    tokens = torch.LongTensor(padded_tokens)\n",
    "    targets = torch.LongTensor(padded_targets)\n",
    "    return tokens, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, test_tokens, train_targets, test_targets = train_test_split(\n",
    "    token_sequences, target_sequences, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequenceDataset(train_tokens, train_targets)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=5,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SequenceDataset(test_tokens, test_targets)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=5,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.hstack(train_targets)\n",
    "a, b = len(stacked) - stacked.sum(), stacked.sum()\n",
    "class_weights = torch.Tensor([b, a]) / (b + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkLabeller(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinkLabeller, self).__init__()\n",
    "        self.backbone = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(768, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, token_sequences):\n",
    "        segments = torch.zeros_like(token_sequences)\n",
    "        x, _ = self.backbone(token_sequences, segments)\n",
    "        return self.head(x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkLabeller().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(weight=class_weights.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for param in list(model.backbone.children())[0].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for module in list(list(list(model.backbone.children())[1].children())[0].children())[\n",
    "    :10\n",
    "]:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        loop = tqdm(train_loader)\n",
    "        for tokens, targets in loop:\n",
    "            tokens = tokens.cuda(non_blocking=True)\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "            segments = torch.zeros_like(tokens)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(tokens)\n",
    "            loss = loss_function(preds.permute(0, 2, 1), targets)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            loop.set_description(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=np.mean(losses[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, loss_function, optimiser, n_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.Series(losses).rolling(window=100).mean()\n",
    "ax = loss_data.plot()\n",
    "ax.set_ylim(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(output_string):\n",
    "    return (\n",
    "        output_string.replace(\"</b> <b>\", \" \")\n",
    "        .replace(\"<b>##\", \"<b>\")\n",
    "        .replace(\" ##\", \"\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(tokens, targets, preds):\n",
    "    target_string, pred_string = \"\", \"\"\n",
    "\n",
    "    for token_id, target, pred in zip(tokens, targets, preds):\n",
    "        token = tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
    "\n",
    "        if target == 1:\n",
    "            target_string += \"<b>\" + token + \"</b> \"\n",
    "        else:\n",
    "            target_string += token + \" \"\n",
    "\n",
    "        if pred == 1:\n",
    "            pred_string += \"<b>\" + token + \"</b> \"\n",
    "        else:\n",
    "            pred_string += token + \" \"\n",
    "\n",
    "    output_string = (\n",
    "        \"PRED:<br>\"\n",
    "        + clean(pred_string)\n",
    "        + \"<br><br>TARG:<br>\"\n",
    "        + clean(target_string)\n",
    "        + \"<br><br>--------<br><br>\"\n",
    "    )\n",
    "\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (tokens, targets) in enumerate(test_loader):\n",
    "        if i < 10:\n",
    "            tokens = tokens  # .cuda()\n",
    "            targets = targets  # .cuda()\n",
    "            segments = torch.zeros_like(tokens)\n",
    "\n",
    "            preds = model(tokens)\n",
    "            preds = nn.LogSoftmax(dim=1)(preds[0]).argmax(dim=1)\n",
    "\n",
    "            output += format_output(tokens[0], targets[0], preds)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/efs/models/20190222_bert_link_labeller.pt\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model for use on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkLabeller()\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, targets = next(iter(test_loader))\n",
    "\n",
    "preds = model(tokens)\n",
    "preds = nn.LogSoftmax(dim=1)(preds[0]).argmax(dim=1)\n",
    "output_html = format_output(tokens[0], targets[0], preds)\n",
    "display(HTML(output_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(tokens)\n",
    "preds = nn.LogSoftmax(dim=1)(preds[0]).argmax(dim=1)\n",
    "output_html = format_output(tokens[0], targets[0], preds)\n",
    "display(HTML(output_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try with new text from wellcome domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Last week I attended a colloquium in Berlin, Das Erbe der Berliner Sexualwissenschaft: Eine Fachtagung sexualwissenschaftlicher Archive, commemorating the 80th anniversary of destruction of Magnus Hirschfeld‘s Institut für Sexualwissenschaft by the Nazis on 6 May 1933.\n",
    "\n",
    "I had been asked to talk about the material we hold in the Wellcome Library relating to Hirschfeld and his legacy and the impact of continental sexual science on British sexologists. There is a small amount of material specifically relating to Hirschfeld in Archives and Manuscripts: like Havelock Ellis, he was a respondent to Dr Josef Strasser’s questionnaire on his career decisions, c. 1930, and his 3-page letter to Strasser and a pamphlet can be found in MS.7042.\n",
    "\n",
    "There is also a group of photographs of the World League for Sexual Reform (founded by Hirschfeld) Congress in Brno, 1932 among the archives of the Family Planning Association. Charlotte Wolff worked with Hirschfeld in her younger days in Berlin, and her papers among the archives of the British Psychological Society include her research files for her 1986 biography of him, the first to be published in English. The Library also holds copies of several of his works.\n",
    "\n",
    "I was also able to mention that we hold the papers of Hirschfeld’s important precursor, Richard von Krafft-Ebing, as well as some material on Havelock Ellis, and important early printed works of sexology, including the first edition of Krafft-Ebing’s Psychopathia Sexualis and the German, and first English, editions of Ellis and J A Symond’s Sexual Inversion (the latter is very rare since Symonds’ executor bought up the entire edition to protect the family from scandal and distress). There is also a significant amount in A&M and the Library more generally pertaining to Hirschfeld’s leading British disciple, the Australian doctor Norman Haire.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text_tokens = tokenizer.tokenize(text)\n",
    "tokens = torch.LongTensor([tokenizer.convert_tokens_to_ids(text_tokens)])\n",
    "\n",
    "preds_continuous = model(tokens)\n",
    "preds = nn.LogSoftmax(dim=1)(preds_continuous[0]).argmax(dim=1)\n",
    "output_html = format_output(tokens[0], torch.zeros_like(tokens[0]), preds)\n",
    "display(HTML(output_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_continuous.detach().numpy().sum(axis=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(preds_continuous.detach().numpy().sum(axis=2)[0]).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
