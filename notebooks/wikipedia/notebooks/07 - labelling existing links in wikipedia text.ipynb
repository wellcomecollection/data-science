{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from fastai.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In learning to link, we want to label our downloaded wikipedia text so that links are easily identifiable. \n",
    "\n",
    "A binary scheme, in which non-link-tokens are labeled `0` and link-tokens are labelled `1`, makes sense for use in a machine learning context, so we'll aim for that.\n",
    "For example, for the sentence:\n",
    "\n",
    "> Traditionally, the term \"philosophy\" referred to any body of [knowledge](https://en.wikipedia.org/wiki/Knowledge).\n",
    "\n",
    "we want to return\n",
    "\n",
    "```\n",
    "0    Traditionally\n",
    "0    ,\n",
    "0    the\n",
    "0    term\n",
    "0    \"\n",
    "0    philosophy\n",
    "0    \"\n",
    "0    referred\n",
    "0    to\n",
    "0    any\n",
    "0    body\n",
    "0    of\n",
    "1    knowledge\n",
    "0    .\n",
    "```\n",
    "\n",
    "First, we need to load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/mnt/efs/wikipedia/dumps/text/AA/wiki_00\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    file = f.read().decode(\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file is made up of multiple articles, so we'll split them by `<doc>` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(?:<doc.+>)((.|\\s|\\S)*?)(?:<\\/doc>)\"\n",
    "docs = [doc[0] for doc in re.findall(pattern, file)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links are still embedded in the text as html, so we need to parse them and pull them each out as text. We then tokenise both link-text and full-text, using the standard `fast.ai` tokeniser. Then, we use the knuth-morris-pratt algorithm to find instances of subsequences (our link texts) in a larger sequence (our full text). These tokens are labelled as 1s and a full labelled doc is returned, ready to be ingested by a neural net / similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_linkable_tokens(article_html, tokenizer=Tokenizer(), label_all=True):\n",
    "    parsed_html = BeautifulSoup(article_html, \"html.parser\")\n",
    "\n",
    "    link_text = [link.text for link in parsed_html.find_all(\"a\")]\n",
    "    tokenised_links = tokenizer.process_all(link_text)\n",
    "\n",
    "    tokenised_text = tokenizer.process_all([parsed_html.text])[0]\n",
    "\n",
    "    target = np.zeros(len(tokenised_text))\n",
    "\n",
    "    for link in tokenised_links:\n",
    "        start_positions = kmp(tokenised_text, link)\n",
    "        if label_all:\n",
    "            for pos in start_positions:\n",
    "                target[pos : pos + len(link)] = 1\n",
    "        elif label_all == False and len(start_positions) > 0:\n",
    "            pos = start_positions[0]\n",
    "            target[pos : pos + len(link)] = 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return tokenised_text, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmp(sequence, sub):\n",
    "    \"\"\"\n",
    "    Knuth–Morris–Pratt algorithm, returning the starting position\n",
    "    of a specified sub within another, larger sequence.\n",
    "    Often used for string matching.\n",
    "    \"\"\"\n",
    "    partial = [0]\n",
    "    for i in range(1, len(sub)):\n",
    "        j = partial[i - 1]\n",
    "        while j > 0 and sub[j] != sub[i]:\n",
    "            j = partial[j - 1]\n",
    "        partial.append(j + 1 if sub[j] == sub[i] else j)\n",
    "\n",
    "    positions, j = [], 0\n",
    "    for i in range(len(sequence)):\n",
    "        while j > 0 and sequence[i] != sub[j]:\n",
    "            j = partial[j - 1]\n",
    "        if sequence[i] == sub[j]:\n",
    "            j += 1\n",
    "        if j == len(sub):\n",
    "            positions.append(i - (j - 1))\n",
    "            j = 0\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    tokenised_text, target = label_linkable_tokens(doc, tokenizer=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(tokenised_text, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we can choose to label _all_ examples of the occurence of a phrase, or the first instance, as is generally the case in wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
