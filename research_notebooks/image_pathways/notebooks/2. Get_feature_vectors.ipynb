{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we:\n",
    "- Load image names from the data pngs saved in `1. Preprocess_images.ipynb`, removing any which have already had feature vectors found (in the S3 'miro-images-feature-vectors' bucket). This step was neccessary since we ran this code over different sessions.\n",
    "- Create a dataset, run dataloader and get feature vectors using the vgg16 pretrained model.\n",
    "- Each feature vector for each image in stored in \"feature_vectors/A0000001\"\n",
    "- We then pull in the feature vectors found in the above step, scale them, take a sample of 5000, and use the elbow method to see how many principle components you can reduce to whilst keeping the explained variance at 1. This value is about 100 components.\n",
    "- We then save dimensionality reduced feature vectors to S3 for these 5000 images, choosing 2, 20, 80, 100, 500, 1000 components. Also saved in the 'miro-images-feature-vectors' bucket under the prefixes \"reduced_feature_vectors_i_dims/A0000001\" where i = 2, 20, 80, 100, 500, 1000.\n",
    "- 120576 images had feature vectors and reduced feature vectors found.\n",
    "\n",
    "Note:\n",
    "- If using an instance with a GPU, this notebook will run using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import vgg\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg.vgg16(pretrained=True).to(device)\n",
    "vgg16 = vgg16.eval()  # for no dropout behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load image names, removing any which have already had feature vectors found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the png image names from the data folder\n",
    "images_dir = '../data/'\n",
    "image_type = '.png'\n",
    "\n",
    "image_names = os.listdir(images_dir)\n",
    "image_names = [os.path.splitext(file)[0] for file in image_names if image_type in file]\n",
    "len(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the image files which already have feature vectors found\n",
    "feat_vect_file_dir = \"feature_vectors\"\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('miro-images-feature-vectors')\n",
    "\n",
    "images_run = [\n",
    "    os.path.basename(file.key) for file in my_bucket.objects.filter(Prefix=feat_vect_file_dir)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [x for x in image_names if x not in images_run]\n",
    "len(image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test showing an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "img = Image.open(images_dir + image_names[i] + image_type)\n",
    "print(type(img))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create dataset, run dataloader and get feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imagesDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, image_names, images_dir, image_type,\n",
    "            transforms=transforms.ToTensor()\n",
    "        ):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.image_names = image_names\n",
    "        self.images_dir = images_dir\n",
    "        self.image_type = image_type\n",
    "        self.index_to_id = {\n",
    "            index: id for index, id in enumerate(self.image_names) \n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.index_to_id[index]\n",
    "        \n",
    "        im = Image.open(\n",
    "            self.images_dir + image_id + self.image_type\n",
    "            )\n",
    "        \n",
    "        img = self.transforms(im)\n",
    "        \n",
    "        image_name = image_id\n",
    "                                      \n",
    "        return image_name, img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the images transforms\n",
    "min_img_size = 224, 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
    "transform_pipeline = transforms.Compose([transforms.Resize(min_img_size),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Remove the last layer from the model, so that the output will be a feature vector\n",
    "vgg16_short = vgg16\n",
    "vgg16_short.classifier = vgg16.classifier[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataloader = DataLoader(\n",
    "    dataset=imagesDataset(\n",
    "        image_names=image_names,\n",
    "        images_dir=images_dir,\n",
    "        image_type=image_type,\n",
    "        transforms=transform_pipeline\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save feature vectors to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "for image_names, images in tqdm(our_dataloader):\n",
    "    images = images.to(device)\n",
    "    feature_vectors = vgg16_short(images)\n",
    "    feature_vectors = feature_vectors.to('cpu')\n",
    "    for image_name, feature_vector in zip(image_names, feature_vectors):\n",
    "        s3.put_object(\n",
    "            Bucket=\"miro-images-feature-vectors\",\n",
    "            Key=\"feature_vectors/\" + image_name,\n",
    "            Body=feature_vector.detach().numpy().tobytes()\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dataloader_test = DataLoader(\n",
    "    dataset=imagesDataset(\n",
    "        image_names=images_run[0:2],\n",
    "        images_dir=images_dir,\n",
    "        image_type=image_type,\n",
    "        transforms=transform_pipeline\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_names, images in tqdm(our_dataloader_test):\n",
    "    images = images.to(device)\n",
    "    feature_vectors = vgg16_short(images)\n",
    "    feature_vectors = feature_vectors.to('cpu')\n",
    "    for image_name, feature_vector in zip(image_names, feature_vectors):\n",
    "        print(image_name)\n",
    "        print(feature_vector[0:10])\n",
    "        print(feature_vector.detach().numpy().tobytes()[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality reduction for a sample of 5000\n",
    "How small can the feature vectors be without losing interesting information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Pull in the feature vectors found in the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://alexwlchan.net/2017/07/listing-s3-keys/\n",
    "def get_all_s3_keys(bucket):\n",
    "    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n",
    "    keys = []\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "    while True:\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            keys.append(obj['Key'])\n",
    "\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'miro-images-feature-vectors'\n",
    "\n",
    "keys = get_all_s3_keys(bucket_name)\n",
    "\n",
    "folder_name = \"feature_vectors\"\n",
    "keys = [k for k in keys if k.split(\"/\")[0]==folder_name]\n",
    "\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = np.random.choice(keys, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = {}\n",
    "for key in tqdm(keys):\n",
    "    obj = s3.get_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=key\n",
    "        )\n",
    "    read_obj = obj['Body'].read()\n",
    "\n",
    "    feature_vectors[key] = np.frombuffer(\n",
    "                    read_obj, dtype=np.float\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors_list = list(feature_vectors.values())\n",
    "feature_vectors_names = list(feature_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_vectors_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. How many dimensions can we reduce to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rescaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_rescaled = scaler.fit_transform(feature_vectors_list)\n",
    "corner_x = 100\n",
    "\n",
    "#Fitting the PCA algorithm with our Data\n",
    "pca = PCA().fit(data_rescaled) # (n_samples, n_features)\n",
    "variance_vals = np.cumsum(pca.explained_variance_ratio_)\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot([0,1000], [1,1], 'r--')\n",
    "plt.plot(variance_vals)\n",
    "plt.plot(corner_x, variance_vals[corner_x],'x')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Feature Vectors Explained Variance')\n",
    "plt.xlim(0,1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure()\n",
    "plt.plot([0,1000], [1,1], 'r--')\n",
    "plt.plot(variance_vals)\n",
    "plt.plot(corner_x, variance_vals[corner_x],'x')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Feature Vectors Explained Variance')\n",
    "plt.xlim(0,1000)\n",
    "plt.savefig('../feat_vec_var.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Save dimensionality reduced feature vectors to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_x = 500\n",
    "pca = PCA(n_components=corner_x)\n",
    "transformed_feature_vectors = pca.fit_transform(data_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "for i, transformed_data in tqdm(enumerate(transformed_feature_vectors)):\n",
    "    image_name = os.path.basename(feature_vectors_names[i])\n",
    "    s3.put_object(\n",
    "        Bucket=\"miro-images-feature-vectors\",\n",
    "        Key=\"reduced_feature_vectors_{}_dims/{}\".format(corner_x, image_name),\n",
    "        Body=bytes(transformed_data)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
