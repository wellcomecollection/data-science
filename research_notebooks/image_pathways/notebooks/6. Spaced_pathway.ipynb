{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we:\n",
    "- Load feature vectors\n",
    "- Use fv_spaced_pathway_nD to find the pathways in reduced data using different umap.UMAP parameters to reduce the data\n",
    "- Use fv_spaced_pathway_nD to find the pathways in original feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from io import BytesIO\n",
    "import ast \n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import compress\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import boto3\n",
    "from scipy.spatial.distance import cdist\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from itertools import combinations\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.network_functions import (\n",
    "    import_feature_vectors,\n",
    "    image_pathway_plot,\n",
    "    reduce_data_nd,\n",
    "    fv_spaced_pathway_nD\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the png image names from the data folder\n",
    "images_dir = 'data/'\n",
    "image_type = '.png'\n",
    "\n",
    "image_names = os.listdir(images_dir)\n",
    "image_names = [os.path.splitext(file)[0] for file in image_names if image_type in file]\n",
    "len(image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sample of images to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 1000\n",
    "np.random.seed(0) # For dev\n",
    "image_name_list = np.random.choice(image_names, n_sample, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get feature vectors as they are (>4000 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'miro-images-feature-vectors'\n",
    "folder_name = \"feature_vectors\"\n",
    "n = 3 # This is what X degrees of separation uses 15, but perhaps this is too much, should it be a fraction of the n_sample?\n",
    "dist_threshold = 0.35 \n",
    "\n",
    "bucket_name = bucket_name\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors, _ = import_feature_vectors(s3, bucket_name, folder_name, image_name_list)\n",
    "\n",
    "# Remove the name of this image from the list if no feature vector was found for it\n",
    "image_name_list = [x for x in image_name_list if x in list(feature_vectors.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names_dict = {k:v for k,v in enumerate(image_name_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experimenting with the umap.UMAP parameters\n",
    "- n_components\n",
    "- min_dist\n",
    "- n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in (2, 50, 200):\n",
    "    for min_dist in (0.1, 0.5, 0.9):\n",
    "        for n_components in (2, 3):\n",
    "            x_data = reduce_data_nd(feature_vectors, n_components, n_neighbors, min_dist)\n",
    "\n",
    "            node1 = 100\n",
    "            node2 = 30\n",
    "            n_nodes = 6\n",
    "\n",
    "            node_path = fv_spaced_pathway_nD(x_data, node1, node2, n_nodes)\n",
    "            image_pathway_plot(\n",
    "                images_dir, image_type, node_path,\n",
    "                title=\"{} components, {} min dist, {} neighbors\".format(n_components, min_dist, n_neighbors)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use all the feature vectors (don't reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors_trans = {k:list(v) for k,v in feature_vectors.items()}\n",
    "\n",
    "node1 = 100\n",
    "node2 = 30\n",
    "n_nodes = 6\n",
    "\n",
    "node_path = fv_spaced_pathway_nD(feature_vectors_trans, node1, node2, n_nodes)\n",
    "image_pathway_plot(\n",
    "    images_dir, image_type, node_path,\n",
    "    title=\"{} components, {} min dist, {} neighbors\".format(n_components, min_dist, n_neighbors)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
